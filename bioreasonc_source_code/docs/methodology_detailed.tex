%% BioREASONIC-Bench: Detailed Methodology Documentation
%% This document provides implementation details for academic use

\section{Benchmark Curation and Reasoning Taxonomies}

BioREASONIC-Bench is designed to evaluate explainable reasoning in biomedical data. Unlike existing benchmarks such as BioKGBench and CARDBiomedBench, which primarily assess knowledge retrieval and question answering, our benchmark targets four complementary reasoning dimensions: \textit{Structure-aware (S)}, \textit{Risk-aware (R)}, \textit{Causal-aware (C)}, and \textit{Mechanism-aware (M)}. The benchmark supports two atomic evaluation abilities: \textit{Knowledge Graph Question Answering (KGQA)}, which tests reasoning over structured causal knowledge graphs using a causal-oriented GraphRAG framework, and \textit{Expert-Level Verification (ELV)}, which evaluates reasoning validity by cross-checking graph-derived evidence against external biomedical resources. An overview of benchmark curation and evaluation is shown in Figure~\ref{fig:BM_overview}.

%% ============================================================================
\subsection{Reasoning Taxonomy Framework}
%% ============================================================================

The four reasoning taxonomies are designed to capture distinct classes of biomedical reasoning, each with specific evaluation criteria and question templates.

\subsubsection{Structure-Aware Reasoning (S)}

Structure-aware reasoning evaluates the ability to traverse and understand biological network structures. This taxonomy operates at three levels of complexity:

\begin{enumerate}
    \item \textbf{One-Hop Reasoning}: Direct mappings such as variant$\rightarrow$gene (e.g., ``Which gene does rs1234567 affect?'') and gene$\rightarrow$disease associations.

    \item \textbf{Multi-Hop Traversal}: Path-based queries involving variant$\rightarrow$gene$\rightarrow$disease connections and SNP$\rightarrow$gene$\rightarrow$pathway$\rightarrow$disease paths.

    \item \textbf{Complex Graph Operations}: N-hop path finding, multi-input convergence queries, and subgraph reasoning tasks.
\end{enumerate}

Question templates for this taxonomy include:
\begin{itemize}
    \item \texttt{S-GENE-MAP}: ``Which gene is the variant \{rsid\} located in or associated with?''
    \item \texttt{S-SNP-GENE}: ``Is the variant \{rsid\} located within or near the \{gene\} gene?''
    \item \texttt{S-CHROM-LOC}: ``On which chromosome is the gene \{gene\} located?''
\end{itemize}

\subsubsection{Risk-Aware Reasoning (R)}

Risk-aware reasoning evaluates the interpretation and aggregation of genetic risk evidence, central to precision medicine applications. The taxonomy encompasses:

\begin{enumerate}
    \item \textbf{Risk Magnitude Interpretation}: Classification of variants into HIGH/MODERATE/LOW risk categories based on odds ratios (OR):
    \begin{itemize}
        \item OR $\geq 2.0$: HIGH risk (strong increase in disease risk)
        \item OR $\geq 1.5$: MODERATE-HIGH risk
        \item OR $\geq 1.2$: MODERATE risk (modest increase)
        \item OR $\geq 1.0$: LOW risk (small increase)
        \item OR $< 0.8$: PROTECTIVE effect
    \end{itemize}

    \item \textbf{Comparative Risk Reasoning}: Ranking variants by inferred risk contribution and comparing relative disease severity.

    \item \textbf{Risk Aggregation}: Gene-level risk profile aggregation from variant-level data using the GRASS scoring framework.
\end{enumerate}

Statistical significance is assessed against the genome-wide threshold ($p < 5 \times 10^{-8}$).

\subsubsection{Causal-Aware Reasoning (C)}

Causal-aware reasoning is the \textbf{core focus} of BioREASONIC-Bench, distinguishing statistical association from true causation. This taxonomy enforces strict causal language discipline:

\begin{table}[h]
\centering
\caption{Causal Language Mapping by Evidence Type}
\begin{tabular}{ll}
\toprule
\textbf{Evidence Type} & \textbf{Appropriate Language} \\
\midrule
GWAS only (no MR) & ``associated with'', ``linked to'', ``risk factor'' \\
& \textit{Never}: ``causes'', ``leads to'', ``results in'' \\
\midrule
MR-supported & ``causal evidence suggests'', ``causally linked'' \\
& With uncertainty quantification \\
\midrule
Functional validation & ``mechanistically established'', ``causally validated'' \\
\bottomrule
\end{tabular}
\end{table}

Question templates include:
\begin{itemize}
    \item \texttt{C-CAUSAL-VS-ASSOC}: ``Is the relationship between \{gene\} and \{disease\} causal or associative based on GWAS evidence?''
    \item \texttt{C-MR-EVIDENCE}: ``What type of evidence would strengthen the causal claim between \{gene\} variation and \{disease\}?''
\end{itemize}

Answers employ Chain-of-Thought (CoT) reasoning to explicitly demonstrate causal reasoning steps.

\subsubsection{Mechanism-Aware Reasoning (M)}

Mechanism-aware reasoning evaluates biological pathway understanding and functional evidence interpretation. This taxonomy comprises two complementary sub-categories:

\paragraph{M1: Biological Mechanism}
Evaluates understanding of protein-protein interaction (PPI) networks, Gene Ontology (GO) terms, and pathway relevance:

\begin{enumerate}
    \item \textbf{Functional Connection Assessment}: Determining whether a gene is functionally connected to disease pathways based on PPI networks and GO term enrichment.

    \item \textbf{GO Term Enrichment Analysis}: Identifying relevant biological processes (e.g., GO:0042593 for glucose homeostasis) that link genes to disease mechanisms.

    \item \textbf{Mechanistic Reasoning}: Tracing biological pathways from gene function to disease phenotype (e.g., GCK $\rightarrow$ glucose sensing $\rightarrow$ insulin secretion $\rightarrow$ T2D).
\end{enumerate}

Question templates include:
\begin{itemize}
    \item \texttt{M-FUNC-CONNECTION}: ``Is \{gene\} functionally connected to \{disease\} pathways based on PPI networks and GO term enrichment?''
    \item \texttt{M-GO-RELEVANCE}: ``Which GO biological process is most relevant for \{gene\}'s role in \{disease\}?''
    \item \texttt{M-PLAUSIBILITY}: ``Evaluate the biological plausibility of \{gene\} as a \{disease\} gene using PPI network and GO term evidence.''
\end{itemize}

\paragraph{M2: Semantic Understanding}
Evaluates biomedical language comprehension and entity recognition:

\begin{itemize}
    \item \textbf{Entity Recognition}: Identification of genes, SNPs, diseases, and statistical measures
    \item \texttt{M-ENTITY-RECOGNIZE}: ``Identify the gene symbol mentioned in: `\{gene\} variant \{rsid\} is associated with \{disease\} susceptibility.'''
    \item \textbf{Relation Extraction}: Extracting structured relationships from biomedical text
    \item \texttt{M-REL-EXTRACT}: ``Extract the biomedical relation from: `Variant \{rsid\} maps to \{gene\} and confers risk for \{disease\}.'''
    \item \textbf{Causal Language Detection}: Distinguishing causal from associative language in biomedical statements
\end{itemize}

%% ============================================================================
\subsection{LLM-Based Benchmark Generation Pipeline}
%% ============================================================================

Benchmark construction follows a scalable LLM-based pipeline consisting of five sequential stages, orchestrated by the \texttt{BioREASONCPipeline} class.

\subsubsection{Stage 1: Question-Answer Generation}

The \texttt{QuestionGenerator} module creates taxonomy-specific question-answer pairs from structured SNP$\rightarrow$Gene$\rightarrow$Disease data sourced from CAUSALdb2.

\paragraph{Input Specification:}
The generator accepts data with the following schema:
\begin{itemize}
    \item \texttt{rsid/SNP/variant}: SNP identifier (e.g., ``rs1234567'')
    \item \texttt{gene/Gene/symbol}: Gene symbol (e.g., ``BRCA1'', ``GCK'')
    \item \texttt{chromosome/chr}: Chromosome number
    \item \texttt{OR/or\_value}: Odds ratio
    \item \texttt{P-Value/p\_value}: Statistical significance
    \item \texttt{disease}: Disease name (context parameter)
\end{itemize}

\paragraph{Generation Process:}
\begin{enumerate}
    \item \textbf{Column Standardization}: Flexible mapping accommodates different data source formats
    \item \textbf{Field Computation}: Derived fields computed (risk\_level, risk\_interpretation, significance\_answer)
    \item \textbf{Template Matching}: For each data row, check template applicability based on required fields
    \item \textbf{Question Generation}: Fill placeholders with entity values
    \item \textbf{Ground Truth Creation}: Deterministic answers with confidence scores and source evidence
    \item \textbf{ID Assignment}: Unique identifiers by taxonomy (e.g., ``C-0001'', ``R-0042'')
\end{enumerate}

\paragraph{Output:}
Each generated item (\texttt{GeneratedItem}) contains:
\begin{itemize}
    \item \texttt{id}: Unique identifier
    \item \texttt{taxonomy}: S, C, R, or M
    \item \texttt{label}: Template label (e.g., ``C-CAUSAL-VS-ASSOC'')
    \item \texttt{question}: Generated question text
    \item \texttt{answer}: Generated answer with evidence
    \item \texttt{answer\_type}: single\_entity, yes\_no, numeric, explanation
    \item \texttt{entities}: Extracted entities \{gene, snp, disease, odds\_ratio, p\_value\}
    \item \texttt{ground\_truth}: Validation data with confidence score
    \item \texttt{difficulty}: easy, medium, hard
    \item \texttt{source\_data}: Original row for traceability
\end{itemize}

\subsubsection{Stage 2: Multi-LLM Validation}

The \texttt{MultiLLMValidator} performs quality assessment using three independent LLM providers to mitigate single-model biases.

\paragraph{Validators Used:}
\begin{itemize}
    \item OpenAI GPT-4o-mini: Fast structured output generation
    \item Anthropic Claude-3-Haiku: Strong reasoning, catches subtle overclaims
    \item Google Gemini-1.5-Flash: Fast with good biomedical knowledge
\end{itemize}

\paragraph{Validation Dimensions (scored 1--5):}
\begin{enumerate}
    \item \textbf{ACCURACY}: Scientific correctness of the answer
    \item \textbf{CLARITY}: Question unambiguity
    \item \textbf{COMPLETENESS}: Answer comprehensiveness
    \item \textbf{REASONING}: Soundness of explanation logic
    \item \textbf{SCIENTIFIC\_VALIDITY}: Consistency with biomedical knowledge
\end{enumerate}

\paragraph{Taxonomy-Specific Criteria:}
\begin{itemize}
    \item \textbf{C (Causal)}: Causal faithfulness---correctly distinguishes association from causation, avoids overclaiming, mentions limitations (confounding, reverse causation), preserves uncertainty language
    \item \textbf{R (Risk)}: Correct OR interpretation, appropriate risk classification, avoids deterministic language
    \item \textbf{M (Mechanism)}: Correct biological pathway reasoning, PPI network interpretation, GO term relevance, entity identification, relationship type extraction
    \item \textbf{S (Structure)}: Accurate gene/SNP mapping, correct chromosomal locations
\end{itemize}

\paragraph{Consensus Calculation:}
\begin{enumerate}
    \item Collect judgments from all validators
    \item Calculate agreement score (1 if unanimous, 0 otherwise)
    \item Determine consensus via majority vote
    \item Compute overclaim consensus for C taxonomy items
\end{enumerate}

\paragraph{Validation Thresholds:}
Items fail validation if:
\begin{itemize}
    \item Average score $< 4.0$
    \item Majority of validators score $< 4.0$ (when \texttt{require\_majority=True})
    \item Overclaim consensus = True (majority detect overclaim)
    \item Ground truth mismatch
\end{itemize}

\subsubsection{Stage 3: Evidence-Grounded Explanation}

The \texttt{ExplanationGenerator} adds concise biomedical explanations ($\sim$35 words) for each validated Q\&A pair.

\paragraph{Purpose:}
\begin{enumerate}
    \item \textbf{Educational}: Help users understand why an answer is correct
    \item \textbf{Evaluation Context}: Provide models with reasoning context during inference
    \item \textbf{Quality Assurance}: Explicit reasoning can be validated for correctness
\end{enumerate}

\paragraph{Generation Methods:}
\begin{enumerate}
    \item \textbf{Primary}: LLM generation (OpenAI $\rightarrow$ Anthropic fallback)
    \item \textbf{Fallback}: Template-based explanations from \texttt{ExplainerPrompts}
\end{enumerate}

\paragraph{Taxonomy-Specific Focus:}
\begin{itemize}
    \item S: Network structure, graph traversal, gene mapping
    \item C: Association vs. causation, MR evidence, confounding
    \item R: Odds ratio interpretation, risk classification, p-values
    \item M: Biological pathways, PPI networks, GO terms, functional mechanisms, entity recognition
\end{itemize}

\subsubsection{Stage 4: Linguistic Paraphrasing}

The \texttt{QuestionParaphraser} generates 2--3 diverse question phrasings to test model robustness.

\paragraph{Purpose:}
\begin{enumerate}
    \item Prevent pattern matching (models should understand meaning, not memorize phrasings)
    \item Test generalization (same answer for different phrasings)
    \item Increase benchmark diversity
    \item Detect overfitting
\end{enumerate}

\paragraph{Entity Preservation:}
Critical entities are preserved exactly (case-sensitive):
\begin{itemize}
    \item Gene names (CAPS): BRCA1, GCK, ACE2, TP53
    \item SNP IDs (rs prefix): rs1799884, rs4607517
    \item Disease names: COVID-19, Type 2 Diabetes
    \item Odds ratios: OR=1.45
    \item P-values: p=5e-8
    \item Risk levels: HIGH, MODERATE, LOW
\end{itemize}

\paragraph{Verification Process:}
\begin{verbatim}
Original:   "Is BRCA1 a risk factor for Breast Cancer?"
Paraphrase: "Does BRCA1 increase risk for Breast Cancer?"
Entities:   ["BRCA1", "Breast Cancer"]
Status:     VALID (both preserved exactly)

Paraphrase: "Does Brca1 increase risk for breast cancer?"
Status:     INVALID (case mismatch) -> fallback to rule-based
\end{verbatim}

\paragraph{Output Format:}
Each paraphrased item receives a modified ID (e.g., ``C-0001-P0'', ``C-0001-P1'') with metadata linking to the original question.

\subsubsection{Stage 5: Benchmark Export}

The \texttt{BenchmarkExporter} splits processed items into train/dev/test sets and exports to standard formats.

\paragraph{Output Files:}
\begin{itemize}
    \item \texttt{train.json}: 70\% of items for model training
    \item \texttt{dev.json}: 15\% of items for validation/tuning
    \item \texttt{test.json}: 15\% of items for final evaluation
    \item \texttt{metadata.json}: Benchmark statistics, generation configuration
    \item \texttt{pipeline\_report.json}: Full pipeline execution report
    \item \texttt{validation\_stats.json}: Validation statistics per taxonomy
    \item \texttt{rejected\_items.json}: Items that failed validation (for analysis)
\end{itemize}

%% ============================================================================
\subsection{Human Expert Evaluation Loop}
%% ============================================================================

The \texttt{HumanExpertEvaluator} module provides a human-in-the-loop evaluation system for validating LLM judgments and providing feedback for iterative improvement.

\subsubsection{Evaluation Workflow}

\begin{enumerate}
    \item \textbf{Export}: Validation results exported to CSV for human expert review
    \item \textbf{Review}: Domain experts evaluate items with columns:
    \begin{itemize}
        \item \texttt{human\_judgment}: ASSOCIATIVE | CAUSAL | NOT\_APPLICABLE
        \item \texttt{human\_is\_overclaim}: Boolean
        \item \texttt{human\_agrees\_with\_llm}: 1 (Agree) or 0 (Disagree)
        \item \texttt{human\_feedback}: Free-text comments
    \end{itemize}
    \item \textbf{Feedback Integration}: Disagreements fed back to generator for improvement
\end{enumerate}

\subsubsection{Feedback Record Structure}

When human experts disagree with LLM judgments, feedback records are created:
\begin{itemize}
    \item \texttt{issue\_type}: OVERCLAIM\_MISSED | FALSE\_POSITIVE\_OVERCLAIM | WRONG\_CAUSAL\_JUDGMENT | OTHER
    \item \texttt{corrected\_judgment}: The correct causal judgment
    \item \texttt{improvement\_hint}: Specific guidance for improvement
    \item \texttt{should\_regenerate}: Boolean flag for regeneration
\end{itemize}

\subsubsection{Regeneration with Feedback}

The \texttt{regenerate\_with\_feedback()} method applies corrections based on issue type:
\begin{itemize}
    \item \textbf{OVERCLAIM\_MISSED}: Convert causal language to associative (``causes'' $\rightarrow$ ``is associated with'')
    \item \textbf{FALSE\_POSITIVE\_OVERCLAIM}: Retain original (may strengthen if needed)
    \item \textbf{WRONG\_CAUSAL\_JUDGMENT}: Adjust language based on corrected judgment
\end{itemize}

%% ============================================================================
\subsection{CARES: Causal-Aware Reasoning Evaluation Score}
%% ============================================================================

CARES is a reasoning-centric evaluation metric that quantifies the alignment between LLM outputs and structured biomedical knowledge, validated causal evidence, and GRASS-weighted genetic risk signals derived from the causal-risk knowledge graph.

\subsubsection{Per-Query Scoring}

Each response is evaluated across the four identified taxonomies. For a query $q$, each taxonomy $\tau \in \mathcal{T} = \{S, R, C, M\}$ receives a discrete score $s_{\tau}^{(q)} \in \{1,2,3,4,5\}$ reflecting:
\begin{itemize}
    \item Quality of entity grounding
    \item Causal validity
    \item Risk-aware interpretation
\end{itemize}

Unsupported causal claims are penalized more strongly in the causal-aware dimension due to the higher risk of biomedical misinterpretation.

\subsubsection{Score Computation}

The per-query CARES score is computed as:
\begin{equation}
\mathrm{CARES}(q) = \frac{1}{4}\left(s_{S}^{(q)} + s_{R}^{(q)} + s_{C}^{(q)} + s_{M}^{(q)}\right)
\end{equation}

For a benchmark set $\mathcal{Q} = \{q_1, \ldots, q_N\}$, the overall CARES score is:
\begin{equation}
\mathrm{CARES}_{\text{overall}} = \frac{1}{N} \sum_{q \in \mathcal{Q}} \mathrm{CARES}(q)
\end{equation}

\subsubsection{Taxonomy-Specific Scoring Criteria}

\paragraph{Structure-Aware (S) Scoring:}
\begin{itemize}
    \item 5: Correct entity mapping with accurate network traversal
    \item 4: Correct mapping with minor path inefficiencies
    \item 3: Partial correctness in multi-hop queries
    \item 2: Significant errors in entity relationships
    \item 1: Incorrect or missing structural understanding
\end{itemize}

\paragraph{Risk-Aware (R) Scoring:}
\begin{itemize}
    \item 5: Correct OR interpretation and risk classification
    \item 4: Correct interpretation with minor precision issues
    \item 3: Partially correct risk assessment
    \item 2: Misclassification of risk level
    \item 1: Incorrect or deterministic risk claims
\end{itemize}

\paragraph{Causal-Aware (C) Scoring:}
\begin{itemize}
    \item 5: Correctly identifies association vs. causation with appropriate uncertainty
    \item 4: Correct judgment with minor language imprecision
    \item 3: Partially correct but missing key causal caveats
    \item 2: Overclaiming causation from associative evidence
    \item 1: Completely incorrect causal judgment or dangerous overclaims
\end{itemize}

\paragraph{Mechanism-Aware (M) Scoring:}
\begin{itemize}
    \item 5: Correct biological plausibility assessment with accurate PPI/GO analysis, proper mechanistic reasoning, and correct entity recognition
    \item 4: Correct pathway interpretation with minor gaps in GO term specificity or entity formatting
    \item 3: Partial mechanistic understanding; identifies relevant pathways but misses key functional connections
    \item 2: Significant errors in pathway reasoning or incorrect biological mechanism attribution
    \item 1: Failed to identify biological mechanisms or completely incorrect pathway interpretation
\end{itemize}

\textit{Note: M taxonomy scoring integrates both biological mechanism (M1) and semantic understanding (M2) components.}

%% ============================================================================
\subsection{Chain-of-Thought (CoT) Answer Templates}
%% ============================================================================

The benchmark employs structured Chain-of-Thought reasoning templates for each taxonomy, enabling explicit step-by-step reasoning evaluation.

\subsubsection{Causal-Aware (C) CoT Templates}

\paragraph{C-CAUSAL-VS-ASSOC Template:}
\begin{verbatim}
The relationship is ASSOCIATIVE, not causal.

**Step 1 - Evidence Type:**
GWAS provides statistical association (OR={or_value}, p={p_value}).

**Step 2 - Causal Limitation:**
GWAS identifies correlation but CANNOT prove causation due to:
- Potential confounding variables
- Possible reverse causation
- Linkage disequilibrium with true causal variant

**Step 3 - Required Evidence for Causality:**
- Mendelian Randomization (MR) analysis
- Functional studies showing biological mechanism
- Intervention trials

**Step 4 - Conclusion:**
Based on GWAS alone, we can only say {gene} is ASSOCIATED with
{disease}, not that it CAUSES {disease}.
\end{verbatim}

\paragraph{C-MR-EVIDENCE Template:}
\begin{verbatim}
To strengthen the causal claim, the following evidence would be needed:

**Step 1 - Mendelian Randomization (MR):**
Use genetic variants as instrumental variables to infer causality
while minimizing confounding.

**Step 2 - Functional Studies:**
Demonstrate biological mechanism showing HOW {gene} variation
affects {disease} pathophysiology.

**Step 3 - Intervention Evidence:**
Clinical trials or pharmacological interventions targeting the pathway.

**Step 4 - Triangulation:**
Multiple independent study designs all pointing to same conclusion.
\end{verbatim}

\subsubsection{Structure-Aware (S) CoT Templates}

\paragraph{Graph Traversal Analysis:}
\begin{verbatim}
Graph Traversal Analysis:

Node 1 (Variant): {rsid}
  | [located_in relationship]
Node 2 (Gene): {gene}
  | [encodes relationship]
Node 3 (Protein): {protein} enzyme
  | [participates_in relationship]
Node 4 (Pathway): {pathway}
  | [dysregulation_causes relationship]
Node 5 (Disease): {disease}

Path length: 4 hops
Relationship types: structural (variant-gene),
                    functional (gene-pathway),
                    causal (pathway-disease)
\end{verbatim}

\subsubsection{Risk-Aware (R) CoT Templates}

\paragraph{Evidence Assessment Framework:}
\begin{verbatim}
Evidence Assessment for {gene} -> {disease}:

1. STATISTICAL EVIDENCE
   - GWAS p-value: {p_value}
   - Effect size: OR = {or_value}
   - Assessment: [STRONG/MODERATE/WEAK] statistical support

2. REPLICATION EVIDENCE
   - Replicated in: [populations]
   - Number of independent studies: [count]
   - Assessment: [STRONG/MODERATE/WEAK] replication

3. CAUSAL EVIDENCE
   - MR score: {mr_score}
   - Direction consistent across MR methods
   - Assessment: [STRONG/MODERATE/INSUFFICIENT] causal evidence

4. BIOLOGICAL PLAUSIBILITY
   - Known function: [description]
   - Monogenic evidence: [if applicable]
   - Assessment: [STRONG/MODERATE/WEAK] biological support

OVERALL CONCLUSION: [Summary statement]
\end{verbatim}

\subsubsection{Mechanism-Aware (M) CoT Templates}

\paragraph{Biological Plausibility Assessment:}
\begin{verbatim}
BIOLOGICAL PLAUSIBILITY ASSESSMENT

1. GO TERM ENRICHMENT ANALYSIS
   Relevant GO terms for {gene}:
   - GO:{id} ({term}) - DIRECT
   - GO:{id} ({term}) - HIGHLY RELEVANT
   - GO:{id} ({term}) - RELEVANT
   Assessment: [STRONG/MODERATE/WEAK] pathway relevance to {disease}

2. PPI NETWORK ANALYSIS
   {gene} interacts with:
   - {interactor1} - [relationship type]
   - {interactor2} - [relationship type]
   - {interactor3} - [relationship type]
   Network proximity to {disease} genes: [HIGH/MEDIUM/LOW]

3. MECHANISTIC REASONING
   {gene} -> [function] -> [intermediate] -> {disease}
   This pathway is [well-established/hypothetical].

CONCLUSION: {gene} has [STRONG/MODERATE/WEAK] biological plausibility
as a {disease} gene based on GO terms, PPI networks, and known function.

NOTE: Biological plausibility supports but does not PROVE causation.
Plausibility + MR evidence together provide strong causal support.
\end{verbatim}

\subsubsection{CoT Template Summary}

\begin{table}[h]
\centering
\caption{Chain-of-Thought Templates by Taxonomy}
\begin{tabular}{llp{6cm}}
\toprule
\textbf{Taxonomy} & \textbf{Template} & \textbf{Purpose} \\
\midrule
S (Structure) & Graph Traversal Analysis & Multi-hop path reasoning through knowledge graph (Variant$\rightarrow$Gene$\rightarrow$Pathway$\rightarrow$Disease) \\
\midrule
C (Causal) & C-CAUSAL-VS-ASSOC & 4-step reasoning for association vs. causation distinction \\
C (Causal) & C-CANNOT-CONCLUDE & 4-step reasoning for why GWAS $\neq$ causation \\
C (Causal) & C-MR-EVIDENCE & 4-step reasoning for strengthening causal claims \\
\midrule
R (Risk) & Evidence Assessment & 4-criterion evaluation (statistical, replication, causal, biological) \\
\midrule
M (Mechanism) & Biological Plausibility & 3-step analysis (GO terms, PPI network, mechanistic reasoning) \\
\bottomrule
\end{tabular}
\end{table}

%% ============================================================================
\subsection{Pipeline Configuration}
%% ============================================================================

The \texttt{PipelineConfig} class controls all pipeline parameters:

\begin{table}[h]
\centering
\caption{Pipeline Configuration Parameters}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Default} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Generator Settings}} \\
\texttt{use\_cot\_answers} & True & Chain-of-Thought answers for C taxonomy \\
\texttt{kg\_path} & None & Path to CAUSALdb2 CSV \\
\texttt{n\_per\_taxonomy} & 100 & Items per taxonomy (S, C, R, M) \\
\texttt{stratify\_by\_evidence} & True & Balance by evidence level \\
\midrule
\multicolumn{3}{l}{\textit{Paraphraser Settings}} \\
\texttt{num\_paraphrases} & 2 & Paraphrases per question (1--3) \\
\texttt{use\_llm\_paraphrase} & True & Use LLM for paraphrasing \\
\midrule
\multicolumn{3}{l}{\textit{Explainer Settings}} \\
\texttt{target\_explanation\_words} & 35 & Target word count \\
\texttt{use\_llm\_explanation} & True & Use LLM for explanation \\
\midrule
\multicolumn{3}{l}{\textit{Validator Settings}} \\
\texttt{passing\_threshold} & 4.0 & Minimum score to pass (1--5) \\
\texttt{require\_majority} & True & Majority must pass \\
\texttt{min\_validators} & 2 & Minimum validators needed \\
\bottomrule
\end{tabular}
\end{table}

%% ============================================================================
\subsection{Implementation Notes}
%% ============================================================================

\subsubsection{API Requirements}
The pipeline requires API keys for at least two LLM providers (recommended: all three):
\begin{itemize}
    \item OpenAI API key (GPT-4o-mini)
    \item Anthropic API key (Claude-3-Haiku)
    \item Google Gemini API key (Gemini-1.5-Flash)
\end{itemize}

\subsubsection{Data Requirements}
Input data should follow CAUSALdb2 format with:
\begin{itemize}
    \item Standard gene symbols (HGNC nomenclature)
    \item Numeric OR values (not confidence intervals)
    \item Scientific notation for p-values
    \item Accurate disease names
\end{itemize}

\subsubsection{Reproducibility}
The pipeline supports:
\begin{itemize}
    \item Checkpoint saving and resumption
    \item Deterministic ground truth generation
    \item Full provenance tracking via \texttt{source\_data} fields
    \item Rejected item logging for quality analysis
\end{itemize}

%% ============================================================================
\subsection{Benchmark Statistics}
%% ============================================================================

\subsubsection{Dataset Size and Distribution}

The BioREASONIC-Bench benchmark comprises 4,450 validated question-answer pairs derived from the CAUSALdb2 knowledge graph.

\begin{table}[h]
\centering
\caption{Knowledge Graph Source Statistics}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total Gene-Disease Pairs & 66,057 \\
Unique Genes & 15,039 \\
Unique Diseases & 544 \\
MR-Validated Pairs & 12,502 \\
Pathway-Supported Pairs & 426 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Benchmark Distribution by Taxonomy}
\begin{tabular}{lrp{7cm}}
\toprule
\textbf{Taxonomy} & \textbf{Count} & \textbf{Description} \\
\midrule
S (Structure) & 1,000 & SNP-Gene mapping, genomic structure, graph traversal \\
C (Causal) & 1,150 & Association vs. causation, MR evidence interpretation \\
R (Risk) & 1,150 & Genetic risk assessment, OR interpretation, comparative risk \\
M (Mechanism) & 1,150 & Biological pathways, PPI networks, GO terms, entity recognition \\
\midrule
\textbf{Total} & \textbf{4,450} & \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Distribution by Answer Format}
\begin{tabular}{lr}
\toprule
\textbf{Format} & \textbf{Count} \\
\midrule
Yes/No & 890 \\
Multiple Choice (A, B, C, D) & 890 \\
Short Answer & 890 \\
Long Answer & 890 \\
Reasoning \& Explanation (CoT) & 890 \\
\midrule
\textbf{Total} & \textbf{4,450} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Distribution by Difficulty and Evidence Level}
\begin{tabular}{lr|lr}
\toprule
\textbf{Difficulty} & \textbf{Count} & \textbf{Evidence Level} & \textbf{Count} \\
\midrule
Easy & 1,850 & Very Strong & 1,515 \\
Medium & 1,900 & Moderate & 1,560 \\
Hard & 700 & Suggestive & 835 \\
 & & Weak & 540 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Train/Dev/Test Split}

The benchmark is split into three subsets for model development and evaluation:
\begin{itemize}
    \item \textbf{Train}: 70\% (3,115 items) -- for model fine-tuning
    \item \textbf{Dev}: 15\% (668 items) -- for hyperparameter tuning and validation
    \item \textbf{Test}: 15\% (667 items) -- for final evaluation (held-out)
\end{itemize}

Each split maintains stratified sampling across taxonomies, difficulty levels, and evidence levels to ensure balanced representation.

%% ============================================================================
%% END OF METHODOLOGY DOCUMENT
%% ============================================================================
