{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# BioResonKGBench: Multi-Model LLM Evaluation\n",
    "\n",
    "**Evaluating 8 LLM Models on Causal Knowledge Graph Question Answering**\n",
    "\n",
    "## Overview\n",
    "\n",
    "BioResonKGBench contains questions across four taxonomies:\n",
    "- **S (Structure)**: Graph navigation and topology\n",
    "- **R (Risk)**: Quantitative risk assessment\n",
    "- **C (Causal)**: Causal evidence evaluation\n",
    "- **M (Mechanism)**: Pathway and semantic understanding\n",
    "\n",
    "## Models Evaluated\n",
    "1. claude-3-haiku\n",
    "2. deepseek-v3\n",
    "3. gpt-4.1\n",
    "4. gpt-4.1-mini\n",
    "5. gpt-4o\n",
    "6. gpt-4o-mini\n",
    "7. llama-3.1-8b\n",
    "8. qwen-2.5-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install openai anthropic neo4j pandas matplotlib seaborn tqdm pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n",
      "============================================================\n",
      "BioResonKGBench Multi-Model Evaluation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Set, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BioResonKGBench Multi-Model Evaluation\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark directory: /ibex/user/alsaedsb/ROCKET/Data/BioREASONIC/benchmarks/02_BioResonKGBench\n",
      "Data directory: /ibex/user/alsaedsb/ROCKET/Data/BioREASONIC/benchmarks/02_BioResonKGBench/data\n",
      "Results directory: /ibex/user/alsaedsb/ROCKET/Data/BioREASONIC/benchmarks/02_BioResonKGBench/results\n",
      "KGQA src directory: /ibex/user/alsaedsb/ROCKET/Data/BioREASONIC/benchmarks/01_BioKGBench/src\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Configure Paths\n",
    "# =============================================================================\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "# Detect project structure\n",
    "if NOTEBOOK_DIR.name == 'tutorials':\n",
    "    BENCHMARK_DIR = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    BENCHMARK_DIR = NOTEBOOK_DIR\n",
    "\n",
    "# BioResonKGBench paths\n",
    "DATA_DIR = BENCHMARK_DIR / 'data'\n",
    "RESULTS_DIR = BENCHMARK_DIR / 'results'\n",
    "CONFIG_DIR = BENCHMARK_DIR / 'config'\n",
    "\n",
    "# BioKGBench src (for KGQA system)\n",
    "BIOKGBENCH_DIR = BENCHMARK_DIR.parent / '01_BioKGBench'\n",
    "SRC_DIR = BIOKGBENCH_DIR / 'src'\n",
    "BIOKGBENCH_CONFIG_DIR = BIOKGBENCH_DIR / 'config'\n",
    "\n",
    "# Add src to path\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# Create results directory if needed\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Benchmark directory: {BENCHMARK_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"KGQA src directory: {SRC_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "API Key Status\n",
      "============================================================\n",
      "OPENAI_API_KEY     Configured (sk-proj-8MD311fkBlQY...)\n",
      "ANTHROPIC_API_KEY  Configured (sk-ant-api03-5zli5oZ...)\n",
      "TOGETHER_API_KEY   Configured (1e793b4f3ba98e902a88...)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load API Keys from Config\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"API Key Status\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try BioResonKGBench config first, then BioKGBench\n",
    "config_path = CONFIG_DIR / 'config.local.yaml'\n",
    "if not config_path.exists():\n",
    "    config_path = BIOKGBENCH_CONFIG_DIR / 'config.local.yaml'\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    llm_config = config.get('llm', {})\n",
    "    \n",
    "    api_keys = {\n",
    "        'OPENAI_API_KEY': llm_config.get('openai', {}).get('api_key'),\n",
    "        'ANTHROPIC_API_KEY': llm_config.get('claude', {}).get('api_key'),\n",
    "        'TOGETHER_API_KEY': llm_config.get('together', {}).get('api_key'),\n",
    "    }\n",
    "    \n",
    "    for key_name, key_value in api_keys.items():\n",
    "        if key_value:\n",
    "            os.environ[key_name] = key_value\n",
    "            status = f\"Configured ({key_value[:20]}...)\"\n",
    "        else:\n",
    "            status = \"Not set\"\n",
    "        print(f\"{key_name:<18} {status}\")\n",
    "else:\n",
    "    print(f\"Config not found: {config_path}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Load BioResonKGBench Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 192 questions from dev set\n",
      "Loaded 1088 questions from test set\n",
      "\n",
      "Dev Set Distribution:\n",
      "  Taxonomy: {'M': 54, 'C': 48, 'S': 42, 'R': 48}\n",
      "  Category: {'knowledge': 114, 'reasoning': 78}\n"
     ]
    }
   ],
   "source": [
    "def load_benchmark_data(split: str = 'dev') -> List[Dict]:\n",
    "    \"\"\"Load benchmark questions from combined JSON file.\"\"\"\n",
    "    file_path = DATA_DIR / f'combined_CKGQA_{split}_matched.json'\n",
    "    with open(file_path, 'r') as f:\n",
    "        questions = json.load(f)\n",
    "    print(f\"Loaded {len(questions)} questions from {split} set\")\n",
    "    return questions\n",
    "\n",
    "# Load data\n",
    "dev_questions = load_benchmark_data('dev')\n",
    "test_questions = load_benchmark_data('test')\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nDev Set Distribution:\")\n",
    "taxonomy_dist = defaultdict(int)\n",
    "category_dist = defaultdict(int)\n",
    "for q in dev_questions:\n",
    "    taxonomy_dist[q.get('taxonomy', 'unknown')] += 1\n",
    "    category_dist[q.get('category', 'unknown')] += 1\n",
    "\n",
    "print(f\"  Taxonomy: {dict(taxonomy_dist)}\")\n",
    "print(f\"  Category: {dict(category_dist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Sample Questions by Taxonomy\n",
      "============================================================\n",
      "\n",
      "[S] S-DISEASE-GENES\n",
      "Q: What genes are risk factors for Coronary Artery Disease?...\n",
      "Type: one-hop\n",
      "\n",
      "[R] R-PVALUE\n",
      "Q: How statistically significant is the association between SNP rs2856690 and Celia...\n",
      "Type: one-hop\n",
      "\n",
      "[C] C-EVIDENCE-LEVEL\n",
      "Q: What is the evidence level for gene CCDC26 affecting Neoplasms?...\n",
      "Type: classification\n",
      "\n",
      "[M] M-PROTEIN\n",
      "Q: What protein does gene PHC3 translate into?...\n",
      "Type: one-hop\n"
     ]
    }
   ],
   "source": [
    "# Show sample questions\n",
    "print(\"=\"*60)\n",
    "print(\"Sample Questions by Taxonomy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for taxonomy in ['S', 'R', 'C', 'M']:\n",
    "    sample = next((q for q in dev_questions if q.get('taxonomy') == taxonomy), None)\n",
    "    if sample:\n",
    "        print(f\"\\n[{taxonomy}] {sample['task_id']}\")\n",
    "        print(f\"Q: {sample['question'][:80]}...\")\n",
    "        print(f\"Type: {sample.get('type', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Test Neo4j Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing Neo4j Connection\n",
      "============================================================\n",
      "Connecting to: bolt://10.73.107.108:7687\n",
      "Connected! Database has 1,006,535 nodes\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Test Neo4j Connection\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Neo4j Connection\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "    \n",
    "    # Load Neo4j config\n",
    "    kg_config_path = BIOKGBENCH_CONFIG_DIR / 'kg_config.yml'\n",
    "    if not kg_config_path.exists():\n",
    "        kg_config_path = CONFIG_DIR / 'kg_config.yml'\n",
    "    \n",
    "    with open(kg_config_path) as f:\n",
    "        neo4j_config = yaml.safe_load(f)\n",
    "    \n",
    "    uri = f\"bolt://{neo4j_config['db_url']}:{neo4j_config['db_port']}\"\n",
    "    print(f\"Connecting to: {uri}\")\n",
    "    \n",
    "    driver = GraphDatabase.driver(\n",
    "        uri,\n",
    "        auth=(neo4j_config['db_user'], neo4j_config['db_password']),\n",
    "        encrypted=False\n",
    "    )\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"MATCH (n) RETURN count(n) as count LIMIT 1\")\n",
    "        count = result.single()['count']\n",
    "        print(f\"Connected! Database has {count:,} nodes\")\n",
    "    \n",
    "    driver.close()\n",
    "    NEO4J_AVAILABLE = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "    NEO4J_AVAILABLE = False\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXED evaluation functions defined!\n",
      "Gold answers will be extracted by running Cypher queries against Neo4j.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FIXED: Gold Answer Extraction - Must run Cypher query!\n",
    "# =============================================================================\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "def get_neo4j_driver():\n",
    "    \"\"\"Get a Neo4j driver connection.\"\"\"\n",
    "    kg_config_path = BIOKGBENCH_CONFIG_DIR / 'kg_config.yml'\n",
    "    if not kg_config_path.exists():\n",
    "        kg_config_path = CONFIG_DIR / 'kg_config.yml'\n",
    "    \n",
    "    with open(kg_config_path) as f:\n",
    "        neo4j_config = yaml.safe_load(f)\n",
    "    \n",
    "    uri = f\"bolt://{neo4j_config['db_url']}:{neo4j_config['db_port']}\"\n",
    "    return GraphDatabase.driver(\n",
    "        uri,\n",
    "        auth=(neo4j_config['db_user'], neo4j_config['db_password']),\n",
    "        encrypted=False\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_gold_answers_from_cypher(question: Dict, driver) -> Set[str]:\n",
    "    \"\"\"\n",
    "    FIXED: Extract gold answers by RUNNING the Cypher query.\n",
    "    The answer is NOT stored in the question - it must be computed!\n",
    "    \"\"\"\n",
    "    answers = set()\n",
    "    \n",
    "    cypher = question.get('cypher', '')\n",
    "    answer_key = question.get('answer_key', '')\n",
    "    \n",
    "    if not cypher or not answer_key:\n",
    "        return answers\n",
    "    \n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(cypher)\n",
    "            records = list(result)\n",
    "            \n",
    "            for record in records:\n",
    "                # Get the answer_key column from results\n",
    "                if answer_key in record.keys():\n",
    "                    value = record[answer_key]\n",
    "                    if value is not None:\n",
    "                        answers.add(normalize_answer(str(value)))\n",
    "                \n",
    "                # Also check common answer columns\n",
    "                for key in ['answer', 'name', 'id', 'gene', 'protein', 'disease', 'snp']:\n",
    "                    if key in record.keys() and key != answer_key:\n",
    "                        value = record[key]\n",
    "                        if value is not None:\n",
    "                            answers.add(normalize_answer(str(value)))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Cypher error: {e}\")\n",
    "    \n",
    "    return answers\n",
    "\n",
    "\n",
    "def normalize_answer(answer: str) -> str:\n",
    "    \"\"\"Normalize answer for comparison.\"\"\"\n",
    "    if answer is None:\n",
    "        return \"\"\n",
    "    return str(answer).strip().lower()\n",
    "\n",
    "\n",
    "def compute_metrics(results: List[Dict]) -> Dict:\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    total = len(results)\n",
    "    if total == 0:\n",
    "        return {'total': 0}\n",
    "    \n",
    "    correct = sum(1 for r in results if r.get('correct', False))\n",
    "    executable = sum(1 for r in results if r.get('success', False))\n",
    "    answered = sum(1 for r in results if r.get('predicted'))\n",
    "    \n",
    "    # By taxonomy\n",
    "    by_taxonomy = defaultdict(lambda: {'total': 0, 'correct': 0})\n",
    "    for r in results:\n",
    "        tax = r.get('taxonomy', 'unknown')\n",
    "        by_taxonomy[tax]['total'] += 1\n",
    "        if r.get('correct', False):\n",
    "            by_taxonomy[tax]['correct'] += 1\n",
    "    \n",
    "    return {\n",
    "        'total': total,\n",
    "        'correct': correct,\n",
    "        'accuracy': correct / total,\n",
    "        'executability': executable / total,\n",
    "        'coverage': answered / total,\n",
    "        'by_taxonomy': {k: v['correct']/v['total'] if v['total'] > 0 else 0 \n",
    "                       for k, v in by_taxonomy.items()}\n",
    "    }\n",
    "\n",
    "print(\"FIXED evaluation functions defined!\")\n",
    "print(\"Gold answers will be extracted by running Cypher queries against Neo4j.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# =============================================================================\n# FIXED: evaluate_model - Uses Cypher-based gold answer extraction\n# =============================================================================\n\ndef evaluate_model(model_name: str, questions: List[Dict], max_questions: int = None) -> Dict:\n    \"\"\"\n    Evaluate a model on BioResonKGBench questions.\n    \n    FIXED: Now properly extracts gold answers by running Cypher queries!\n    Uses cached gold answers if available (_gold_answers field).\n    \n    Args:\n        model_name: Name of model (e.g., 'gpt-4o-mini')\n        questions: List of question dictionaries\n        max_questions: Limit number of questions (for testing)\n    \"\"\"\n    from kg_qa_system_v2 import KnowledgeGraphQAv2, QAMode\n    \n    print(f\"\\nEvaluating: {model_name}\")\n    \n    # Load config\n    config_path = BIOKGBENCH_CONFIG_DIR / 'config.local.yaml'\n    with open(config_path) as f:\n        config = yaml.safe_load(f)\n    config['llm']['provider'] = model_name\n    \n    # Save temp config\n    temp_config = f'/tmp/bioresonkg_config_{model_name}.yaml'\n    with open(temp_config, 'w') as f:\n        yaml.dump(config, f)\n    \n    # Initialize KGQA system for LLM predictions\n    qa = KnowledgeGraphQAv2(config_path=temp_config, mode=QAMode.LLM)\n    qa.connect()\n    \n    # Get Neo4j driver for gold answer extraction (if not cached)\n    gold_driver = None\n    \n    # Process questions\n    results = []\n    questions_to_eval = questions[:max_questions] if max_questions else questions\n    start_time = time.time()\n    \n    for q in tqdm(questions_to_eval, desc=model_name):\n        question_text = q.get('question', '')\n        taxonomy = q.get('taxonomy', 'unknown')\n        task_id = q.get('task_id', '')\n        \n        # Use cached gold answers if available, otherwise extract\n        if '_gold_answers' in q:\n            gold_answers = q['_gold_answers']\n        else:\n            if gold_driver is None:\n                gold_driver = get_neo4j_driver()\n            gold_answers = extract_gold_answers_from_cypher(q, gold_driver)\n        \n        try:\n            # Get LLM prediction\n            result = qa.answer(question_text, q)\n            predicted = []\n            \n            if result.success and result.answers:\n                for ans in result.answers:\n                    if ans.name:\n                        predicted.append(normalize_answer(ans.name))\n                    if ans.id:\n                        predicted.append(normalize_answer(ans.id))\n                    # Also add extra fields\n                    if ans.extra:\n                        for v in ans.extra.values():\n                            if v is not None:\n                                predicted.append(normalize_answer(str(v)))\n            \n            # Check correctness - flexible matching\n            correct = False\n            if predicted and gold_answers:\n                for p in predicted:\n                    if not p:\n                        continue\n                    if p in gold_answers:\n                        correct = True\n                        break\n                    # Partial match\n                    for g in gold_answers:\n                        if g and (g in p or p in g):\n                            correct = True\n                            break\n                    if correct:\n                        break\n            \n            results.append({\n                'question': question_text,\n                'taxonomy': taxonomy,\n                'task_id': task_id,\n                'gold_answers': list(gold_answers)[:5],  # Limit for readability\n                'predicted': predicted[:5],\n                'correct': correct,\n                'success': result.success,\n                'cypher_generated': result.cypher_query,\n            })\n            \n        except Exception as e:\n            results.append({\n                'question': question_text,\n                'taxonomy': taxonomy,\n                'task_id': task_id,\n                'gold_answers': list(gold_answers)[:5] if gold_answers else [],\n                'predicted': [],\n                'correct': False,\n                'success': False,\n                'error': str(e),\n            })\n    \n    qa.close()\n    if gold_driver:\n        gold_driver.close()\n    total_time = time.time() - start_time\n    \n    metrics = compute_metrics(results)\n    metrics['model'] = model_name\n    metrics['total_time_sec'] = total_time\n    \n    # Show some examples\n    correct_examples = [r for r in results if r['correct']][:2]\n    wrong_examples = [r for r in results if not r['correct'] and r['success']][:2]\n    \n    print(f\"\\nResults: Accuracy={metrics['accuracy']*100:.1f}%, Exec={metrics['executability']*100:.1f}%, Time={total_time:.1f}s\")\n    \n    if correct_examples:\n        print(f\"\\nCorrect example:\")\n        ex = correct_examples[0]\n        print(f\"  Q: {ex['question'][:50]}...\")\n        print(f\"  Gold: {ex['gold_answers'][:3]}\")\n        print(f\"  Pred: {ex['predicted'][:3]}\")\n    \n    if wrong_examples:\n        print(f\"\\nWrong example:\")\n        ex = wrong_examples[0]\n        print(f\"  Q: {ex['question'][:50]}...\")\n        print(f\"  Gold: {ex['gold_answers'][:3]}\")\n        print(f\"  Pred: {ex['predicted'][:3]}\")\n    \n    return {'metrics': metrics, 'results': results}\n\nprint(\"FIXED evaluate_model() function defined!\")\nprint(\"Now properly compares LLM answers with Cypher-derived gold answers.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3edks8moyn",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFYING GOLD ANSWER EXTRACTION (via Cypher)\n",
      "================================================================================\n",
      "\n",
      "Testing gold answer extraction on sample questions:\n",
      "\n",
      "Q1: What protein does gene PHC3 translate into?...\n",
      "    Task: M-PROTEIN\n",
      "    Answer Key: protein_name\n",
      "    Gold Answers: ['phc3']\n",
      "    Count: 1 answers\n",
      "\n",
      "Q2: What is the evidence level for gene CCDC26 affecting Neoplas...\n",
      "    Task: C-EVIDENCE-LEVEL\n",
      "    Answer Key: evidence_level\n",
      "    Gold Answers: []\n",
      "    Count: 0 answers\n",
      "\n",
      "Q3: Which SNP has the strongest causal effect on Malabsorption S...\n",
      "    Task: C-TOP-CAUSAL\n",
      "    Answer Key: snp\n",
      "    Gold Answers: []\n",
      "    Count: 0 answers\n",
      "\n",
      "Q4: What is the evidence level for gene GEMIN7-AS1 affecting Aut...\n",
      "    Task: C-EVIDENCE-LEVEL\n",
      "    Answer Key: evidence_level\n",
      "    Gold Answers: []\n",
      "    Count: 0 answers\n",
      "\n",
      "Q5: What genes are risk factors for Coronary Artery Disease?...\n",
      "    Task: S-DISEASE-GENES\n",
      "    Answer Key: gene\n",
      "    Gold Answers: []\n",
      "    Count: 0 answers\n",
      "\n",
      "================================================================================\n",
      "If gold answers are extracted correctly, the evaluation should now work!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFY: Test Gold Answer Extraction\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFYING GOLD ANSWER EXTRACTION (via Cypher)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test on a few sample questions\n",
    "test_driver = get_neo4j_driver()\n",
    "\n",
    "print(\"\\nTesting gold answer extraction on sample questions:\\n\")\n",
    "\n",
    "for i, q in enumerate(dev_questions[:5]):\n",
    "    print(f\"Q{i+1}: {q['question'][:60]}...\")\n",
    "    print(f\"    Task: {q['task_id']}\")\n",
    "    print(f\"    Answer Key: {q.get('answer_key', 'N/A')}\")\n",
    "    \n",
    "    gold = extract_gold_answers_from_cypher(q, test_driver)\n",
    "    print(f\"    Gold Answers: {list(gold)[:3]}{'...' if len(gold) > 3 else ''}\")\n",
    "    print(f\"    Count: {len(gold)} answers\")\n",
    "    print()\n",
    "\n",
    "test_driver.close()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"If gold answers are extracted correctly, the evaluation should now work!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Run Multi-Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# =============================================================================\n# FIXED: Multi-Model Evaluation with Pre-Filtering\n# =============================================================================\n\nprint(\"=\"*60)\nprint(\"Multi-Model Evaluation on BioResonKGBench\")\nprint(\"=\"*60)\n\n# Step 1: Pre-filter questions to only those with valid gold answers\nprint(\"\\nStep 1: Pre-filtering questions with valid gold answers...\")\n\nprefilter_driver = get_neo4j_driver()\nvalid_questions = []\ninvalid_count = 0\n\nfor q in tqdm(dev_questions, desc=\"Pre-filtering\"):\n    gold = extract_gold_answers_from_cypher(q, prefilter_driver)\n    if gold:  # Only keep questions with extractable gold answers\n        q['_gold_answers'] = gold  # Cache the gold answers\n        valid_questions.append(q)\n    else:\n        invalid_count += 1\n\nprefilter_driver.close()\n\nprint(f\"\\nPre-filter Results:\")\nprint(f\"  Valid questions (with gold answers): {len(valid_questions)}\")\nprint(f\"  Invalid questions (no gold answers): {invalid_count}\")\nprint(f\"  Coverage: {len(valid_questions)/len(dev_questions)*100:.1f}%\")\n\n# Show distribution of valid questions\nvalid_taxonomy = {}\nfor q in valid_questions:\n    tax = q.get('taxonomy', 'unknown')\n    valid_taxonomy[tax] = valid_taxonomy.get(tax, 0) + 1\nprint(f\"  By Taxonomy: {valid_taxonomy}\")\n\n# Define all models to evaluate\nMODELS_TO_TEST = [\n    'gpt-4o-mini',      # Start with fastest/cheapest\n    'gpt-4o',\n    'claude-3-haiku',\n    'llama-3.1-8b',\n    'qwen-2.5-7b',\n    'deepseek-v3',\n    'gpt-4.1-mini',\n    'gpt-4.1',\n]\n\n# Configuration\nMAX_QUESTIONS = 20  # Set to None for full evaluation on valid questions\nDATASET = valid_questions  # Use only valid questions!\n\nprint(f\"\\n{'='*60}\")\nprint(f\"EVALUATION CONFIGURATION\")\nprint(f\"{'='*60}\")\nprint(f\"Models to test: {len(MODELS_TO_TEST)}\")\nprint(f\"Valid questions available: {len(DATASET)}\")\nprint(f\"Questions per model: {MAX_QUESTIONS if MAX_QUESTIONS else 'All'}\")\nprint(\"=\"*60)\n\nif not NEO4J_AVAILABLE:\n    print(\"Neo4j not available. Cannot run evaluation.\")\nelif len(DATASET) == 0:\n    print(\"No valid questions found. Check KG schema alignment.\")\nelse:\n    all_model_results = {}\n    \n    for model_name in MODELS_TO_TEST:\n        print(f\"\\n{'='*60}\")\n        print(f\"Evaluating: {model_name}\")\n        print(f\"{'='*60}\")\n        \n        try:\n            result = evaluate_model(model_name, DATASET, max_questions=MAX_QUESTIONS)\n            all_model_results[model_name] = result\n            print(f\"Completed: {model_name}\")\n        except Exception as e:\n            print(f\"Failed: {model_name} - {e}\")\n            import traceback\n            traceback.print_exc()\n            all_model_results[model_name] = {'error': str(e)}\n    \n    # Summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"EVALUATION SUMMARY\")\n    print(\"=\"*80)\n    print(f\"{'Model':<16} {'Accuracy':>10} {'Exec':>10} {'Time (s)':>10}\")\n    print(\"-\"*80)\n    \n    for model_name in MODELS_TO_TEST:\n        if model_name in all_model_results and 'metrics' in all_model_results[model_name]:\n            m = all_model_results[model_name]['metrics']\n            print(f\"{model_name:<16} {m['accuracy']*100:>9.1f}% {m['executability']*100:>9.1f}% {m.get('total_time_sec', 0):>10.1f}\")\n        else:\n            err = all_model_results.get(model_name, {}).get('error', 'Unknown')\n            print(f\"{model_name:<16} {'FAILED':>10} - {err[:30]}\")\n    \n    print(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Save Results and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Save Results and Create Visualizations\n",
    "# =============================================================================\n",
    "\n",
    "if 'all_model_results' in dir() and all_model_results:\n",
    "    # Create DataFrame\n",
    "    results_data = []\n",
    "    for model_name, result in all_model_results.items():\n",
    "        if 'metrics' in result:\n",
    "            m = result['metrics']\n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Accuracy (%)': m['accuracy'] * 100,\n",
    "                'Executability (%)': m['executability'] * 100,\n",
    "                'Coverage (%)': m['coverage'] * 100,\n",
    "                'Time (s)': m.get('total_time_sec', 0),\n",
    "            }\n",
    "            # Add per-taxonomy accuracy\n",
    "            for tax, acc in m.get('by_taxonomy', {}).items():\n",
    "                row[f'{tax} Acc (%)'] = acc * 100\n",
    "            results_data.append(row)\n",
    "    \n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    df_results = df_results.sort_values('Accuracy (%)', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    print(df_results.to_string())\n",
    "    \n",
    "    # Save to files\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    csv_path = RESULTS_DIR / f'multimodel_eval_{timestamp}.csv'\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nSaved to: {csv_path}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Accuracy comparison\n",
    "    df_plot = df_results.sort_values('Accuracy (%)', ascending=True)\n",
    "    axes[0].barh(df_plot['Model'], df_plot['Accuracy (%)'], color='steelblue')\n",
    "    axes[0].set_xlabel('Accuracy (%)')\n",
    "    axes[0].set_title('Model Accuracy on BioResonKGBench')\n",
    "    axes[0].set_xlim(0, 100)\n",
    "    \n",
    "    # Plot 2: Per-taxonomy heatmap\n",
    "    tax_cols = [c for c in df_results.columns if 'Acc (%)' in c and c != 'Accuracy (%)']\n",
    "    if tax_cols:\n",
    "        heatmap_data = df_results.set_index('Model')[tax_cols]\n",
    "        sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='YlOrRd', ax=axes[1])\n",
    "        axes[1].set_title('Accuracy by Taxonomy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = RESULTS_DIR / f'multimodel_comparison_{timestamp}.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to: {plot_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"No results to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Detailed Analysis by Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Detailed Analysis by Taxonomy\n",
    "# =============================================================================\n",
    "\n",
    "if 'all_model_results' in dir() and all_model_results:\n",
    "    print(\"=\"*80)\n",
    "    print(\"DETAILED ANALYSIS BY TAXONOMY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for taxonomy in ['S', 'R', 'C', 'M']:\n",
    "        print(f\"\\n--- {taxonomy} (Structure/Risk/Causal/Mechanism) ---\")\n",
    "        \n",
    "        for model_name, result in all_model_results.items():\n",
    "            if 'metrics' not in result:\n",
    "                continue\n",
    "            \n",
    "            tax_results = [r for r in result['results'] if r.get('taxonomy') == taxonomy]\n",
    "            if tax_results:\n",
    "                correct = sum(1 for r in tax_results if r.get('correct', False))\n",
    "                total = len(tax_results)\n",
    "                acc = correct / total * 100 if total > 0 else 0\n",
    "                print(f\"  {model_name:<16}: {acc:5.1f}% ({correct}/{total})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook evaluated 8 LLM models on BioResonKGBench:\n",
    "\n",
    "1. **claude-3-haiku** - Anthropic's fast model\n",
    "2. **deepseek-v3** - DeepSeek's latest model\n",
    "3. **gpt-4.1** - OpenAI GPT-4 Turbo\n",
    "4. **gpt-4.1-mini** - OpenAI GPT-4 Turbo Preview\n",
    "5. **gpt-4o** - OpenAI GPT-4o\n",
    "6. **gpt-4o-mini** - OpenAI GPT-4o Mini\n",
    "7. **llama-3.1-8b** - Meta's Llama 3.1 8B\n",
    "8. **qwen-2.5-7b** - Alibaba's Qwen 2.5 7B\n",
    "\n",
    "### Key Metrics\n",
    "- **Accuracy**: Percentage of questions answered correctly\n",
    "- **Executability**: Percentage of queries that executed without error\n",
    "- **Per-Taxonomy**: S (Structure), R (Risk), C (Causal), M (Mechanism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981dcf2a-045b-4748-a88c-4031a647925f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fd3e7-6abd-46ea-9308-114986df82e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222bd93-30a2-423a-89c3-a6a86e734a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ecbe7-3760-4039-b84e-abb6318c0923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}